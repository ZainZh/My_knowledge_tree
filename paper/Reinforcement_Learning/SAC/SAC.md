[TOC]

# Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor



> paper link: https://arxiv.org/pdf/1801.01290.pdf

- [研究摘要](#研究摘要)

- [介绍](#介绍)

- [准备知识](#准备知识)
  
- [CQL框架](#CQL框架)

- [offline RL 的CQL 框架](#offline RL 的CQL 框架)

  

   ## 研究摘要

   model-free(无模型)的强化学习帮助解决了许多控制任务和挑战决策。但是在两个方面存在这很大的挑战：处理高复杂度模型和模型收敛时的脆弱性。所以在本篇文章中，作者提出了一个基于最大熵的强化学习框架。这个框架的目的是最大化奖励的同时能够最大化熵，这样的话就可以尽可能的让任务执行过程中进行随机探索。和其他off-policy的算法相比，我们这个算法更加稳定。

   

   ## 介绍

   offline RL 能解决 online RL中与环境交互下花费高同时又危险的问题，而且online RL可收集到的数据数量要比offline RL少很多。但是offline RL存在的主要问题就是从数据集中学习到的策略与数据集收集的策略存在很大的偏移（训练的结果与理想不同），产生了分布外操作（out-of-distribution）和过拟合（overfitting）。而在策略拟合和策略提升过程中，通常只用到了策略的值Q。所以我们就可以学习一个不那么保守的Q值策略来让这个策略低于下界，然后在通过加入一个数据分布上的最大化项来收紧这个界限。

   

   ## 准备知识

   在马尔科夫决策过程中，我们都希望能够奖励最大化。如果我们定义数据集为 $D$, $\pi_\beta (a|s)$  定义为行为策略（所有策略的合集）。数据集D是从 $d^{\pi\beta}$ $\pi_\beta (a|s)$ 中采集到的。而与此对应的是 $\hat{\pi} _\beta (a|s)$ 是从数据集 $D$ 中学习到的学习到的经验行为策略。因为数据集不可能包含所有的 $(s,a)$ 映射。所以从数据集中训练得到的 $|r(s|a)| \leq R_{max} $ 。

   通常，在Q-learning 方法中，我们通过对贝尔曼最优算子（Func 1）的迭代来得到一个能使Q值最大的策略，然后在通过贪心算法（将这个策略带回去找action,在找能使该action发生概率最大的策略）来不断优化这个策略

   <img src="pics/image-20220629144025682.png" alt="image-20220629144025682" style="zoom: 30%;" />

   ​                                                  Func 1. 贝尔曼最优算子，reward加上系数 $y$  乘 能使接下来的动作Q值最大的 S-R映射的期望。

   

   通常，在 Actor-critic 方法中，有一个单独的策略来训练使 Q值最大。再通过贝尔曼算子对策略进行提升。提升的方式就是通过不断地更新action来最大化期望的Q值。

   <img src="pics/image-20220629145054800.png" alt="image-20220629145054800" style="zoom:30%;" />

   <img src="pics/image-20220629145109108.png" alt="image-20220629145109108" style="zoom:30%;" />

   因为我们是在数据集 $D$ 中进行训练，而 $D$ 不可能包含所有的 $ (s,a,s^{'})$ 映射，所有我们得到的策略是一种基于数据集的经验策略 $ \hat{\pi}^k$。但是在策略改进的过程中，因为为了获得最大的Q值，就有可能错误的指向到可以获得高Q值得分布外行为。

   

   ## CQL 框架

   所以为了阻止策略值的过估计，本文通过Q值最小化并加上了一个标准的贝尔曼误差 来设计了一个十分保守的低下界的Q-function。所以我们选择是在特定的state-action分布 $\mu(s,a)$ 下最小化期望的Q值。因为在Q-function训练不会检索未观测到的states,但是会检索未见到的action。 所以我们把 $\mu$ 限制在数据集的边缘状态。   $\mu(s,a)=$  $d^{\pi\beta}$ $\mu (a|s)$  

   <img src="pics/tempsnip.png" alt="tempsnip" style="zoom:40%;" />

   红框为最小化的Q值函数。绿框为标准贝尔曼误差。 因为我们最小化了Q值函数，所以可以保证在边界内。

   而我们希望能够更逼近边界而不仅仅是在边界内。所我们加了一个在行为策略下的额外的Q值最大化项来希望逼近边界。

   <img src="pics/image-20220629152937404.png" alt="image-20220629152937404" style="zoom: 40%;" />

   因为等式二最大化了行为策略下的Q值。所以得到的Q值可能并不是在边界内，但是通过后续附录证明可以得到基于此产生的策略是低于下界的。

   总结来说，CQL框架通过等式一来学习一个在真实Q值下界内的Q-function,通过等式二获得了一个能够获得期望Q值的更逼近下界的策略。

   ## offline RL 的CQL 框架

   根据上两式，可以通过使 $\mu=\pi$ 来解出能使Q值逼近下界Q的策略 $\pi$， 我们不再通过用一步策略评估获得策略，然后再用一步策略提升改进策略。取而代之的是。 我们用我们从Fun 2中获得策略 $\pi$ 来获得最大的 Q值。 也可以加一个正则项变成CQL（R)

   <img src="pics/image-20220629154529726.png" alt="image-20220629154529726" style="zoom:67%;" />
